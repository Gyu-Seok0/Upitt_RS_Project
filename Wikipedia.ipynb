{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c61ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import requests\n",
    "import json\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d639f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csv_file</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iir-12.1.1.csv</td>\n",
       "      <td>[Language Models, Language Model, Information ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iir-4.6.csv</td>\n",
       "      <td>[Nonpositional Indexes, Nonpositional Index, T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iir-6.csv</td>\n",
       "      <td>[Scoring, Term Weighting, Vector Space Model, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iir-14.2.csv</td>\n",
       "      <td>[Rocchio Classification, Decision Boundaries, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iir-8.1.csv</td>\n",
       "      <td>[Document Collection, Information Needs, Infor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         csv_file                                           keywords\n",
       "0  iir-12.1.1.csv  [Language Models, Language Model, Information ...\n",
       "1     iir-4.6.csv  [Nonpositional Indexes, Nonpositional Index, T...\n",
       "2       iir-6.csv  [Scoring, Term Weighting, Vector Space Model, ...\n",
       "3    iir-14.2.csv  [Rocchio Classification, Decision Boundaries, ...\n",
       "4     iir-8.1.csv  [Document Collection, Information Needs, Infor..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ck_df = pd.read_csv(\"./csv_keywords_df.csv\")\n",
    "ck_df[\"keywords\"] = ck_df['keywords'].apply(lambda x: ast.literal_eval(x))\n",
    "ck_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82533294",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = requests.Session()\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "PARAMS = {\"action\":\"query\", \n",
    "          \"format\":\"json\",\n",
    "          \"list\":\"search\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0787220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_cosine_similarity(sen1, sen2):\n",
    "    \n",
    "    sent = (sen1, sen2)\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sent)\n",
    "    \n",
    "    return cosine_similarity(tfidf_matrix[0:1],tfidf_matrix[1:2])\n",
    "\n",
    "\n",
    "api_token = \"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\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def get_STS(sen1, sen2):\n",
    "    data = query(\n",
    "        {\n",
    "            \"inputs\": {\n",
    "                \"source_sentence\": sen1,\n",
    "                \"sentences\":[sen2]\n",
    "            }\n",
    "        })\n",
    "    return data[0] if type(data) == list else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e77f2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "def cleaning(keywords):\n",
    "    after_ps = set(map(lambda x: ps.stem(x), keywords))\n",
    "    after_ps_lm = set(map(lambda x: lm.lemmatize(x), after_ps))\n",
    "    \n",
    "    return list(after_ps_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d23594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#row.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b627e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning(row.keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b07e41d",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9056b88e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csv_file</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iir-12.1.1.csv</td>\n",
       "      <td>[Language Models, Language Model, Information ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iir-4.6.csv</td>\n",
       "      <td>[Nonpositional Indexes, Nonpositional Index, T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iir-6.csv</td>\n",
       "      <td>[Scoring, Term Weighting, Vector Space Model, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iir-14.2.csv</td>\n",
       "      <td>[Rocchio Classification, Decision Boundaries, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iir-8.1.csv</td>\n",
       "      <td>[Document Collection, Information Needs, Infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>iir-13.1.csv</td>\n",
       "      <td>[Text Classification Problem, Document Space, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>iir-1.4.csv</td>\n",
       "      <td>[Boolean Retrieval Model, Ranked Retrieval Mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>iir-11.2.csv</td>\n",
       "      <td>[Probability Ranking Principle, Collection Of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>iir-4.1.csv</td>\n",
       "      <td>[Index Construction, Inverted Index, Indexing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>iir-7.3.csv</td>\n",
       "      <td>[Vector Space Scoring, Vector Space Score, Que...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          csv_file                                           keywords\n",
       "0   iir-12.1.1.csv  [Language Models, Language Model, Information ...\n",
       "1      iir-4.6.csv  [Nonpositional Indexes, Nonpositional Index, T...\n",
       "2        iir-6.csv  [Scoring, Term Weighting, Vector Space Model, ...\n",
       "3     iir-14.2.csv  [Rocchio Classification, Decision Boundaries, ...\n",
       "4      iir-8.1.csv  [Document Collection, Information Needs, Infor...\n",
       "..             ...                                                ...\n",
       "81    iir-13.1.csv  [Text Classification Problem, Document Space, ...\n",
       "82     iir-1.4.csv  [Boolean Retrieval Model, Ranked Retrieval Mod...\n",
       "83    iir-11.2.csv  [Probability Ranking Principle, Collection Of ...\n",
       "84     iir-4.1.csv  [Index Construction, Inverted Index, Indexing,...\n",
       "85     iir-7.3.csv  [Vector Space Scoring, Vector Space Score, Que...\n",
       "\n",
       "[86 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ck_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df83acbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csv_file</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iir-12.1.1.csv</td>\n",
       "      <td>[Language Models, Language Model, Information ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iir-4.6.csv</td>\n",
       "      <td>[Nonpositional Indexes, Nonpositional Index, T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iir-6.csv</td>\n",
       "      <td>[Scoring, Term Weighting, Vector Space Model, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iir-14.2.csv</td>\n",
       "      <td>[Rocchio Classification, Decision Boundaries, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iir-8.1.csv</td>\n",
       "      <td>[Document Collection, Information Needs, Infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>iir-13.1.csv</td>\n",
       "      <td>[Text Classification Problem, Document Space, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>iir-1.4.csv</td>\n",
       "      <td>[Boolean Retrieval Model, Ranked Retrieval Mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>iir-11.2.csv</td>\n",
       "      <td>[Probability Ranking Principle, Collection Of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>iir-4.1.csv</td>\n",
       "      <td>[Index Construction, Inverted Index, Indexing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>iir-7.3.csv</td>\n",
       "      <td>[Vector Space Scoring, Vector Space Score, Que...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          csv_file                                           keywords\n",
       "0   iir-12.1.1.csv  [Language Models, Language Model, Information ...\n",
       "1      iir-4.6.csv  [Nonpositional Indexes, Nonpositional Index, T...\n",
       "2        iir-6.csv  [Scoring, Term Weighting, Vector Space Model, ...\n",
       "3     iir-14.2.csv  [Rocchio Classification, Decision Boundaries, ...\n",
       "4      iir-8.1.csv  [Document Collection, Information Needs, Infor...\n",
       "..             ...                                                ...\n",
       "81    iir-13.1.csv  [Text Classification Problem, Document Space, ...\n",
       "82     iir-1.4.csv  [Boolean Retrieval Model, Ranked Retrieval Mod...\n",
       "83    iir-11.2.csv  [Probability Ranking Principle, Collection Of ...\n",
       "84     iir-4.1.csv  [Index Construction, Inverted Index, Indexing,...\n",
       "85     iir-7.3.csv  [Vector Space Scoring, Vector Space Score, Que...\n",
       "\n",
       "[86 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ck_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1180bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dict = {}\n",
    "csv_dict = {}\n",
    "csv2wiki = {}\n",
    "wiki2csv = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a2f829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword = probability distribut, target = Distribution, score= 0.5703443884849548\n",
      "keyword = language modeling approach, target = Language model, score= 0.8747274279594421\n",
      "keyword = language of the automaton, target = Automata theory, score= 0.6899217367172241\n",
      "keyword = term, target = Term, score= 1.0000001192092896\n",
      "keyword = ir, target = IR, score= 1.0000001192092896\n",
      "keyword = string, target = String, score= 1.000000238418579\n",
      "keyword = document model, target = Document Object Model, score= 0.915020227432251\n",
      "keyword = probabilistic finite automaton, target = Probabilistic automaton, score= 0.8992546200752258\n",
      "keyword = query likelihood model, target = Query likelihood model, score= 1.0\n",
      "keyword = document, target = Document, score= 0.9999999403953552\n",
      "keyword = generative model, target = Generative model, score= 1.0\n",
      "keyword = likelihood ratio, target = Likelihood-ratio test, score= 0.8607867956161499\n",
      "keyword = document rank, target = Military rank, score= 0.5313839316368103\n",
      "keyword = language model, target = Language model, score= 1.0000001192092896\n",
      "keyword = probabilistic language model, target = Probabilistic programming, score= 0.5322674512863159\n",
      "keyword = probabilistic approach, target = Probabilistic Turing machine, score= 0.6031939387321472\n",
      "keyword = finite automaton, target = Finite-state machine, score= 0.6053465604782104\n",
      "keyword = language theori, target = Theoris, score= 0.6014848351478577\n",
      "keyword = finite automata, target = Finite-state machine, score= 0.6777979135513306\n",
      "keyword = index, target = Index, score= 1.000000238418579\n",
      "keyword = ranked retrieval system, target = Information retrieval, score= 0.6678628325462341\n",
      "keyword = file system, target = File system, score= 1.0000001192092896\n",
      "There is no target, continue\n",
      "keyword = term, target = Term, score= 1.0000001192092896\n",
      "keyword = inverted index, target = Inverted index, score= 0.9999998807907104\n",
      "keyword = post, target = Post, score= 1.0000001192092896\n",
      "keyword = user-document matrix, target = Document-term matrix, score= 0.7397477626800537\n",
      "keyword = algorithm, target = Algorithm, score= 1.0000001192092896\n",
      "There is no target, continue\n",
      "keyword = token, target = Token, score= 1.0000001192092896\n",
      "keyword = query process, target = Query language, score= 0.6545735597610474\n",
      "keyword = secur, target = Securly, score= 0.9010899066925049\n",
      "keyword = access control list, target = Access-control list, score= 0.9868500232696533\n",
      "keyword = retrieval system, target = Information retrieval, score= 0.7677127718925476\n",
      "keyword = document, target = Document, score= 0.9999999403953552\n",
      "keyword = information retrieval system, target = Information retrieval, score= 0.8670783638954163\n",
      "keyword = user membership, target = Member, score= 0.678115725517273\n",
      "keyword = acl, target = ACL, score= 1.0000001192092896\n",
      "keyword = query term, target = Web query, score= 0.6084033250808716\n",
      "keyword = vector space model, target = Vector space model, score= 1.0000001192092896\n",
      "keyword = term-weight, target = Weight, score= 0.5905053615570068\n",
      "keyword = index, target = Index, score= 1.000000238418579\n",
      "keyword = document, target = Document, score= 0.9999999403953552\n",
      "keyword = term weight, target = Weight, score= 0.633842945098877\n",
      "There is no target, continue\n",
      "keyword = score, target = Score, score= 1.0000001192092896\n",
      "keyword = metadata, target = Metadata, score= 1.0\n",
      "keyword = term, target = Term, score= 1.0000001192092896\n",
      "keyword = document collect, target = Collecting, score= 0.6457347869873047\n",
      "keyword = multimodal class, target = Multimodality, score= 0.7559378743171692\n",
      "keyword = relevance feedback, target = Relevance feedback, score= 1.0000001192092896\n",
      "keyword = naive bay, target = Naive Bayes classifier, score= 0.7934862375259399\n",
      "There is no target, continue\n",
      "keyword = rocchio relevance feedback, target = Relevance feedback, score= 0.7148259282112122\n",
      "keyword = algorithm, target = Algorithm, score= 1.0000001192092896\n",
      "keyword = centroid, target = Centroid, score= 1.0000001192092896\n",
      "keyword = knn, target = KNN, score= 0.9999999403953552\n",
      "keyword = rocchio algorithm, target = Rocchio algorithm, score= 1.0\n",
      "keyword = normalized vector, target = Unit vector, score= 0.5696183443069458\n",
      "keyword = cosine similar, target = Cosine similarity, score= 0.9048939347267151\n",
      "keyword = collect, target = Collect, score= 1.0000001192092896\n",
      "keyword = document, target = Document, score= 0.9999999403953552\n",
      "There is no target, continue\n",
      "keyword = rocchio, target = Rocchio algorithm, score= 0.6952248215675354\n",
      "keyword = rocchio formula, target = Rocchio algorithm, score= 0.7183497548103333\n",
      "keyword = normal vector, target = Normal (geometry), score= 0.6230970025062561\n",
      "There is no target, continue\n",
      "keyword = rocchio classification centroid, target = Nearest centroid classifier, score= 0.638238251209259\n",
      "There is no target, continue\n",
      "keyword = class, target = Class, score= 1.0\n",
      "There is no target, continue\n",
      "keyword = ground truth, target = Ground truth, score= 1.0\n",
      "There is no target, continue\n",
      "keyword = development test collect, target = Software testing, score= 0.5816522240638733\n",
      "keyword = web search engin, target = Personalized search, score= 0.5267810225486755\n",
      "keyword = information ne, target = .ne, score= 0.5066636204719543\n",
      "keyword = collect, target = Collect, score= 1.0000001192092896\n",
      "keyword = document collect, target = Collecting, score= 0.6457347869873047\n",
      "keyword = document, target = Document, score= 0.9999999403953552\n",
      "keyword = ad hoc information retrieval effect, target = Relevance feedback, score= 0.5886876583099365\n",
      "There is no target, continue\n",
      "keyword = gold standard, target = Gold standard, score= 1.0000001192092896\n",
      "keyword = query term, target = Web query, score= 0.6084033250808716\n",
      "keyword = index, target = Index, score= 1.000000238418579\n",
      "keyword = cach, target = CACH, score= 1.0000001192092896\n",
      "keyword = score, target = Score, score= 1.0000001192092896\n",
      "keyword = term, target = Term, score= 1.0000001192092896\n",
      "There is no target, continue\n",
      "keyword = crawler, target = Web crawler, score= 0.8089004755020142\n",
      "keyword = rank, target = Rank, score= 1.0000001192092896\n",
      "keyword = natural language process, target = Natural language processing, score= 0.7992518544197083\n",
      "keyword = snippet, target = Snippet, score= 0.9999998807907104\n",
      "keyword = html page, target = HTML, score= 0.8600140810012817\n",
      "There is no target, continue\n",
      "keyword = kwic, target = KWIC, score= 1.0\n",
      "There is no target, continue\n",
      "keyword = collect, target = Collect, score= 1.0000001192092896\n",
      "There is no target, continue\n",
      "There is no target, continue\n",
      "keyword = metadata, target = Metadata, score= 1.0\n",
      "keyword = document, target = Document, score= 0.9999999403953552\n",
      "keyword = keyword-in-context, target = Key Word in Context, score= 0.8949824571609497\n",
      "keyword = zone, target = Zone, score= 1.0\n",
      "keyword = information ne, target = .ne, score= 0.5066636204719543\n",
      "There is no target, continue\n",
      "There is no target, continue\n",
      "keyword = flat clustering algorithm, target = K-means clustering, score= 0.5904055833816528\n",
      "keyword = cluster, target = Cluster, score= 0.9999999403953552\n",
      "keyword = akaike information criterion, target = Akaike information criterion, score= 1.0000001192092896\n",
      "There is no target, continue\n",
      "keyword = data set, target = Data set, score= 1.0000001192092896\n",
      "keyword = centroid, target = Centroid, score= 1.0000001192092896\n",
      "keyword = negative maximum log-likelihood, target = Maximum likelihood estimation, score= 0.6695478558540344\n",
      "keyword = heuristic method, target = Heuristic, score= 0.8505352139472961\n",
      "keyword = document, target = Document, score= 0.9999999403953552\n",
      "keyword = distort, target = Distorted, score= 0.6977219581604004\n",
      "keyword = aic, target = AIC, score= 1.0\n",
      "keyword = r, target = R, score= 1.0000001192092896\n",
      "keyword = training data, target = Training, validation, and test data sets, score= 0.6873860359191895\n",
      "There is no target, continue\n",
      "keyword = term, target = Term, score= 1.0000001192092896\n",
      "keyword = uniform prior, target = Prior probability, score= 0.6663883328437805\n",
      "keyword = multinomial naive bay, target = Naive Bayes classifier, score= 0.7478554248809814\n",
      "There is no target, continue\n",
      "There is no target, continue\n",
      "There is no target, continue\n",
      "keyword = token, target = Token, score= 1.0000001192092896\n",
      "keyword = training set, target = Training, validation, and test data sets, score= 0.5819345116615295\n",
      "There is no target, continue\n",
      "keyword = term weight, target = Weight, score= 0.633842945098877\n",
      "keyword = maximum a posteriori, target = Maximum a posteriori estimation, score= 0.8142149448394775\n",
      "keyword = nb, target = NB, score= 1.0\n",
      "keyword = document, target = Document, score= 0.9999999403953552\n",
      "keyword = add-one smooth, target = Additive smoothing, score= 0.6423196792602539\n",
      "keyword = supervised learning method, target = Supervised learning, score= 0.8721861839294434\n",
      "keyword = spar, target = Spar, score= 1.0000001192092896\n",
      "There is no target, continue\n",
      "keyword = text classification method, target = Document classification, score= 0.7890002727508545\n",
      "keyword = probabilistic learning method, target = Probabilistic classification, score= 0.7893722057342529\n",
      "keyword = stop word, target = Stop word, score= 1.0000001192092896\n",
      "keyword = language model, target = Language model, score= 1.0000001192092896\n",
      "keyword = multinomial naive bayes model, target = Naive Bayes classifier, score= 0.7450528144836426\n",
      "keyword = class, target = Class, score= 1.0\n",
      "keyword = grammar-based language model, target = Language model, score= 0.7777420282363892\n",
      "keyword = higher-order model, target = Higher-order logic, score= 0.6473260521888733\n",
      "keyword = training data, target = Training, validation, and test data sets, score= 0.6873860359191895\n",
      "keyword = unigram language model, target = Language model, score= 0.7574960589408875\n",
      "keyword = term, target = Term, score= 1.0000001192092896\n",
      "There is no target, continue\n",
      "keyword = bias-variance tradeoff, target = Bias–variance tradeoff, score= 0.9932581186294556\n",
      "keyword = ir, target = IR, score= 1.0000001192092896\n",
      "keyword = bigram language model, target = Language model, score= 0.6737626791000366\n",
      "keyword = document, target = Document, score= 0.9999999403953552\n",
      "keyword = probabilistic context-free grammar, target = Probabilistic context-free grammar, score= 1.0000001192092896\n",
      "keyword = language model, target = Language model, score= 1.0000001192092896\n",
      "keyword = ir system, target = IR, score= 0.7390596866607666\n",
      "keyword = geometric mean, target = Geometric mean, score= 1.0\n",
      "keyword = machine learning classification problem, target = Statistical classification, score= 0.6740443110466003\n",
      "keyword = hard disk, target = Hard disk drive, score= 0.9478616118431091\n",
      "keyword = arithmetic mean, target = Arithmetic mean, score= 1.0\n",
      "keyword = document, target = Document, score= 0.9999999403953552\n",
      "keyword = precis, target = Précis, score= 1.000000238418579\n",
      "There is no target, continue\n",
      "keyword = information retrieval system, target = Information retrieval, score= 0.8670783638954163\n",
      "keyword = information retrieval problem, target = Information retrieval, score= 0.8088284730911255\n",
      "There is no target, continue\n",
      "keyword = f1, target = Formula One, score= 0.546417236328125\n",
      "keyword = feature selection algorithm, target = Feature selection, score= 0.8941949009895325\n",
      "keyword = effect, target = Effect, score= 1.000000238418579\n",
      "keyword = greedy method, target = Greedy algorithm, score= 0.8705811500549316\n",
      "There is no target, continue\n",
      "keyword = term, target = Term, score= 1.0000001192092896\n",
      "keyword = non-greedy method, target = Greedy algorithm, score= 0.8353102207183838\n",
      "keyword = degree of freedom, target = Degrees of freedom, score= 0.9322702288627625\n",
      "keyword = bias-variance tradeoff, target = Bias–variance tradeoff, score= 0.9932581186294556\n",
      "There is no target, continue\n",
      "keyword = yates correct, target = Yates's correction for continuity, score= 0.6091315150260925\n",
      "keyword = greedy feature select, target = Feature selection, score= 0.6752206683158875\n",
      "There is no target, continue\n",
      "keyword = mi, target = MI, score= 1.0000001192092896\n",
      "keyword = training set, target = Training, validation, and test data sets, score= 0.5819345116615295\n",
      "keyword = mutual inform, target = Information content, score= 0.5069635510444641\n",
      "keyword = overfit, target = Overfitting, score= 0.7121830582618713\n",
      "There is no target, continue\n",
      "keyword = multinomial model, target = Multinomial logistic regression, score= 0.8166395425796509\n",
      "keyword = feature select, target = Select (SQL), score= 0.5891337394714355\n",
      "keyword = mle, target = MLE, score= 1.0000001192092896\n",
      "keyword = text classification problem, target = Document classification, score= 0.7464883923530579\n",
      "keyword = nb, target = NB, score= 1.0\n",
      "There is no target, continue\n",
      "keyword = collect, target = Collect, score= 1.0000001192092896\n",
      "keyword = document, target = Document, score= 0.9999999403953552\n",
      "There is no target, continue\n",
      "There is no target, continue\n",
      "keyword = learning method, target = Reinforcement learning, score= 0.5799144506454468\n",
      "There is no target, continue\n",
      "keyword = feature selection method, target = Feature selection, score= 0.9015445113182068\n",
      "keyword = class, target = Class, score= 1.0\n",
      "There is no target, continue\n",
      "keyword = training data, target = Training, validation, and test data sets, score= 0.6873860359191895\n",
      "keyword = classifier effect, target = Chinese classifier, score= 0.5188095569610596\n",
      "keyword = outlier, target = Outlier, score= 1.0000001192092896\n",
      "keyword = vector space based machine learning method, target = Support vector machine, score= 0.6571350693702698\n",
      "keyword = svm, target = SVM, score= 1.000000238418579\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'dict' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get Score, continue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mscore\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, target = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, score= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# save\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'dict' and 'float'"
     ]
    }
   ],
   "source": [
    "for row in ck_df.itertuples():\n",
    "    \n",
    "    clean_keyword = cleaning(row.keywords) # check1\n",
    "    \n",
    "    for keyword in clean_keyword:\n",
    "        \n",
    "        # request\n",
    "        PARAMS[\"srsearch\"] = keyword\n",
    "        R = S.get(url = URL, params = PARAMS)\n",
    "        DATA = R.json()\n",
    "        \n",
    "        try:\n",
    "            target = DATA['query']['search'][0]['title']\n",
    "    \n",
    "        except:\n",
    "            print(\"There is no target, continue\")\n",
    "            continue\n",
    "        \n",
    "        # get simiarity score\n",
    "        try:\n",
    "            score = get_STS(keyword, target) #check2\n",
    "        except:\n",
    "            print(\"Cannot get Score, continue\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        \n",
    "        if score >= 0.5:\n",
    "            print(f\"keyword = {keyword}, target = {target}, score= {score}\")\n",
    "            \n",
    "            # save\n",
    "            \n",
    "            if row.csv_file not in csv_dict.keys():\n",
    "                csv_dict[row.csv_file] = len(csv_dict)\n",
    "            \n",
    "            if target not in wiki_dict.keys():\n",
    "                wiki_dict[target] = len(wiki_dict)\n",
    "            \n",
    "            key = csv_dict[row.csv_file]\n",
    "            value = wiki_dict[target]\n",
    "            \n",
    "            if key not in csv2wiki.keys():\n",
    "                csv2wiki[key] = [value]\n",
    "            else:\n",
    "                csv2wiki[key].append(value)\n",
    "            \n",
    "            if value not in wiki2csv.keys():\n",
    "                wiki2csv[value] = [key]\n",
    "            else:\n",
    "                wiki2csv[value].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1282188e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4573d184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iir-12.1.1.csv': 0,\n",
       " 'iir-4.6.csv': 1,\n",
       " 'iir-6.csv': 2,\n",
       " 'iir-14.2.csv': 3,\n",
       " 'iir-8.1.csv': 4,\n",
       " 'iir-8.7.csv': 5,\n",
       " 'iir-16.4.1.csv': 6,\n",
       " 'iir-13.2.csv': 7,\n",
       " 'iir-12.1.2.csv': 8,\n",
       " 'iir-8.3.csv': 9,\n",
       " 'iir-13.5.csv': 10,\n",
       " 'iir-15.csv': 11}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8242920d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Distribution': 0,\n",
       " 'Language model': 1,\n",
       " 'Automata theory': 2,\n",
       " 'Term': 3,\n",
       " 'IR': 4,\n",
       " 'String': 5,\n",
       " 'Document Object Model': 6,\n",
       " 'Probabilistic automaton': 7,\n",
       " 'Query likelihood model': 8,\n",
       " 'Document': 9,\n",
       " 'Generative model': 10,\n",
       " 'Likelihood-ratio test': 11,\n",
       " 'Military rank': 12,\n",
       " 'Probabilistic programming': 13,\n",
       " 'Probabilistic Turing machine': 14,\n",
       " 'Finite-state machine': 15,\n",
       " 'Theoris': 16,\n",
       " 'Index': 17,\n",
       " 'Information retrieval': 18,\n",
       " 'File system': 19,\n",
       " 'Inverted index': 20,\n",
       " 'Post': 21,\n",
       " 'Document-term matrix': 22,\n",
       " 'Algorithm': 23,\n",
       " 'Token': 24,\n",
       " 'Query language': 25,\n",
       " 'Securly': 26,\n",
       " 'Access-control list': 27,\n",
       " 'Member': 28,\n",
       " 'ACL': 29,\n",
       " 'Web query': 30,\n",
       " 'Vector space model': 31,\n",
       " 'Weight': 32,\n",
       " 'Score': 33,\n",
       " 'Metadata': 34,\n",
       " 'Collecting': 35,\n",
       " 'Multimodality': 36,\n",
       " 'Relevance feedback': 37,\n",
       " 'Naive Bayes classifier': 38,\n",
       " 'Centroid': 39,\n",
       " 'KNN': 40,\n",
       " 'Rocchio algorithm': 41,\n",
       " 'Unit vector': 42,\n",
       " 'Cosine similarity': 43,\n",
       " 'Collect': 44,\n",
       " 'Normal (geometry)': 45,\n",
       " 'Nearest centroid classifier': 46,\n",
       " 'Class': 47,\n",
       " 'Ground truth': 48,\n",
       " 'Software testing': 49,\n",
       " 'Personalized search': 50,\n",
       " '.ne': 51,\n",
       " 'Gold standard': 52,\n",
       " 'CACH': 53,\n",
       " 'Web crawler': 54,\n",
       " 'Rank': 55,\n",
       " 'Natural language processing': 56,\n",
       " 'Snippet': 57,\n",
       " 'HTML': 58,\n",
       " 'KWIC': 59,\n",
       " 'Key Word in Context': 60,\n",
       " 'Zone': 61,\n",
       " 'K-means clustering': 62,\n",
       " 'Cluster': 63,\n",
       " 'Akaike information criterion': 64,\n",
       " 'Data set': 65,\n",
       " 'Maximum likelihood estimation': 66,\n",
       " 'Heuristic': 67,\n",
       " 'Distorted': 68,\n",
       " 'AIC': 69,\n",
       " 'R': 70,\n",
       " 'Training, validation, and test data sets': 71,\n",
       " 'Prior probability': 72,\n",
       " 'Maximum a posteriori estimation': 73,\n",
       " 'NB': 74,\n",
       " 'Additive smoothing': 75,\n",
       " 'Supervised learning': 76,\n",
       " 'Spar': 77,\n",
       " 'Document classification': 78,\n",
       " 'Probabilistic classification': 79,\n",
       " 'Stop word': 80,\n",
       " 'Higher-order logic': 81,\n",
       " 'Bias–variance tradeoff': 82,\n",
       " 'Probabilistic context-free grammar': 83,\n",
       " 'Geometric mean': 84,\n",
       " 'Statistical classification': 85,\n",
       " 'Hard disk drive': 86,\n",
       " 'Arithmetic mean': 87,\n",
       " 'Précis': 88,\n",
       " 'Formula One': 89,\n",
       " 'Feature selection': 90,\n",
       " 'Effect': 91,\n",
       " 'Greedy algorithm': 92,\n",
       " 'Degrees of freedom': 93,\n",
       " \"Yates's correction for continuity\": 94,\n",
       " 'MI': 95,\n",
       " 'Information content': 96,\n",
       " 'Overfitting': 97,\n",
       " 'Multinomial logistic regression': 98,\n",
       " 'Select (SQL)': 99,\n",
       " 'MLE': 100,\n",
       " 'Reinforcement learning': 101,\n",
       " 'Chinese classifier': 102,\n",
       " 'Outlier': 103,\n",
       " 'Support vector machine': 104,\n",
       " 'SVM': 105}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dict\n",
    "# address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54dc9a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 13, 14, 15, 16, 15],\n",
       " 1: [17, 18, 19, 3, 20, 21, 22, 23, 24, 25, 26, 27, 18, 9, 18, 28, 29],\n",
       " 2: [30, 31, 32, 17, 9, 32, 33, 34, 3, 35],\n",
       " 3: [36, 37, 38, 37, 23, 39, 40, 41, 42, 43, 44, 9, 41, 41, 45, 46, 47],\n",
       " 4: [48, 49, 50, 51, 44, 35, 9, 37, 52],\n",
       " 5: [30, 17, 53, 33, 3, 54, 55, 56, 57, 58, 59, 44, 34, 9, 60, 61, 51],\n",
       " 6: [62, 63, 64, 65, 39, 66, 67, 9, 68, 69, 70],\n",
       " 7: [71, 3, 72, 38, 24, 71, 32, 73, 74, 9, 75, 76, 77, 78, 79, 80, 1, 38, 47],\n",
       " 8: [1, 81, 71, 1, 3, 82, 4, 1, 9, 83, 1],\n",
       " 9: [4, 84, 85, 86, 87, 9, 88, 18, 18],\n",
       " 10: [89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  3,\n",
       "  92,\n",
       "  93,\n",
       "  82,\n",
       "  94,\n",
       "  90,\n",
       "  95,\n",
       "  71,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  78,\n",
       "  74,\n",
       "  44,\n",
       "  9,\n",
       "  101,\n",
       "  90,\n",
       "  47],\n",
       " 11: [71, 102, 103, 104, 105]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv2wiki \n",
    "# key id -> csv, value id -> wiki\n",
    "# There are some frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38eb954e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0],\n",
       " 1: [0, 0, 7, 8, 8, 8, 8],\n",
       " 2: [0],\n",
       " 3: [0, 1, 2, 5, 7, 8, 10],\n",
       " 4: [0, 8, 9],\n",
       " 5: [0],\n",
       " 6: [0],\n",
       " 7: [0],\n",
       " 8: [0],\n",
       " 9: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 10: [0],\n",
       " 11: [0],\n",
       " 12: [0],\n",
       " 13: [0],\n",
       " 14: [0],\n",
       " 15: [0, 0],\n",
       " 16: [0],\n",
       " 17: [1, 2, 5],\n",
       " 18: [1, 1, 1, 9, 9],\n",
       " 19: [1],\n",
       " 20: [1],\n",
       " 21: [1],\n",
       " 22: [1],\n",
       " 23: [1, 3],\n",
       " 24: [1, 7],\n",
       " 25: [1],\n",
       " 26: [1],\n",
       " 27: [1],\n",
       " 28: [1],\n",
       " 29: [1],\n",
       " 30: [2, 5],\n",
       " 31: [2],\n",
       " 32: [2, 2, 7],\n",
       " 33: [2, 5],\n",
       " 34: [2, 5],\n",
       " 35: [2, 4],\n",
       " 36: [3],\n",
       " 37: [3, 3, 4],\n",
       " 38: [3, 7, 7],\n",
       " 39: [3, 6],\n",
       " 40: [3],\n",
       " 41: [3, 3, 3],\n",
       " 42: [3],\n",
       " 43: [3],\n",
       " 44: [3, 4, 5, 10],\n",
       " 45: [3],\n",
       " 46: [3],\n",
       " 47: [3, 7, 10],\n",
       " 48: [4],\n",
       " 49: [4],\n",
       " 50: [4],\n",
       " 51: [4, 5],\n",
       " 52: [4],\n",
       " 53: [5],\n",
       " 54: [5],\n",
       " 55: [5],\n",
       " 56: [5],\n",
       " 57: [5],\n",
       " 58: [5],\n",
       " 59: [5],\n",
       " 60: [5],\n",
       " 61: [5],\n",
       " 62: [6],\n",
       " 63: [6],\n",
       " 64: [6],\n",
       " 65: [6],\n",
       " 66: [6],\n",
       " 67: [6],\n",
       " 68: [6],\n",
       " 69: [6],\n",
       " 70: [6],\n",
       " 71: [7, 7, 8, 10, 11],\n",
       " 72: [7],\n",
       " 73: [7],\n",
       " 74: [7, 10],\n",
       " 75: [7],\n",
       " 76: [7],\n",
       " 77: [7],\n",
       " 78: [7, 10],\n",
       " 79: [7],\n",
       " 80: [7],\n",
       " 81: [8],\n",
       " 82: [8, 10],\n",
       " 83: [8],\n",
       " 84: [9],\n",
       " 85: [9],\n",
       " 86: [9],\n",
       " 87: [9],\n",
       " 88: [9],\n",
       " 89: [10],\n",
       " 90: [10, 10, 10],\n",
       " 91: [10],\n",
       " 92: [10, 10],\n",
       " 93: [10],\n",
       " 94: [10],\n",
       " 95: [10],\n",
       " 96: [10],\n",
       " 97: [10],\n",
       " 98: [10],\n",
       " 99: [10],\n",
       " 100: [10],\n",
       " 101: [10],\n",
       " 102: [11],\n",
       " 103: [11],\n",
       " 104: [11],\n",
       " 105: [11]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki2csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79194ed9",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee2dd8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6c97a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pickle(name:str):\n",
    "    with open(f'{name}.pickle','wb') as f:\n",
    "        pickle.dump(eval(name), f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "for name in [\"wiki_dict\",\"csv_dict\",\"csv2wiki\",\"wiki2csv\"]:\n",
    "    save_as_pickle(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd593d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wiki2csv.pickle\", 'rb') as f:\n",
    "    t = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc31a22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0],\n",
       " 1: [0, 0, 7, 8, 8, 8, 8],\n",
       " 2: [0],\n",
       " 3: [0, 1, 2, 5, 7, 8, 10],\n",
       " 4: [0, 8, 9],\n",
       " 5: [0],\n",
       " 6: [0],\n",
       " 7: [0],\n",
       " 8: [0],\n",
       " 9: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 10: [0],\n",
       " 11: [0],\n",
       " 12: [0],\n",
       " 13: [0],\n",
       " 14: [0],\n",
       " 15: [0, 0],\n",
       " 16: [0],\n",
       " 17: [1, 2, 5],\n",
       " 18: [1, 1, 1, 9, 9],\n",
       " 19: [1],\n",
       " 20: [1],\n",
       " 21: [1],\n",
       " 22: [1],\n",
       " 23: [1, 3],\n",
       " 24: [1, 7],\n",
       " 25: [1],\n",
       " 26: [1],\n",
       " 27: [1],\n",
       " 28: [1],\n",
       " 29: [1],\n",
       " 30: [2, 5],\n",
       " 31: [2],\n",
       " 32: [2, 2, 7],\n",
       " 33: [2, 5],\n",
       " 34: [2, 5],\n",
       " 35: [2, 4],\n",
       " 36: [3],\n",
       " 37: [3, 3, 4],\n",
       " 38: [3, 7, 7],\n",
       " 39: [3, 6],\n",
       " 40: [3],\n",
       " 41: [3, 3, 3],\n",
       " 42: [3],\n",
       " 43: [3],\n",
       " 44: [3, 4, 5, 10],\n",
       " 45: [3],\n",
       " 46: [3],\n",
       " 47: [3, 7, 10],\n",
       " 48: [4],\n",
       " 49: [4],\n",
       " 50: [4],\n",
       " 51: [4, 5],\n",
       " 52: [4],\n",
       " 53: [5],\n",
       " 54: [5],\n",
       " 55: [5],\n",
       " 56: [5],\n",
       " 57: [5],\n",
       " 58: [5],\n",
       " 59: [5],\n",
       " 60: [5],\n",
       " 61: [5],\n",
       " 62: [6],\n",
       " 63: [6],\n",
       " 64: [6],\n",
       " 65: [6],\n",
       " 66: [6],\n",
       " 67: [6],\n",
       " 68: [6],\n",
       " 69: [6],\n",
       " 70: [6],\n",
       " 71: [7, 7, 8, 10, 11],\n",
       " 72: [7],\n",
       " 73: [7],\n",
       " 74: [7, 10],\n",
       " 75: [7],\n",
       " 76: [7],\n",
       " 77: [7],\n",
       " 78: [7, 10],\n",
       " 79: [7],\n",
       " 80: [7],\n",
       " 81: [8],\n",
       " 82: [8, 10],\n",
       " 83: [8],\n",
       " 84: [9],\n",
       " 85: [9],\n",
       " 86: [9],\n",
       " 87: [9],\n",
       " 88: [9],\n",
       " 89: [10],\n",
       " 90: [10, 10, 10],\n",
       " 91: [10],\n",
       " 92: [10, 10],\n",
       " 93: [10],\n",
       " 94: [10],\n",
       " 95: [10],\n",
       " 96: [10],\n",
       " 97: [10],\n",
       " 98: [10],\n",
       " 99: [10],\n",
       " 100: [10],\n",
       " 101: [10],\n",
       " 102: [11],\n",
       " 103: [11],\n",
       " 104: [11],\n",
       " 105: [11]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a47104",
   "metadata": {},
   "source": [
    "# Ablation: not using cleaning dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5223a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "not using cleaning dataset\n",
    "\n",
    "'''\n",
    "\n",
    "for row in ck_df.itertuples():\n",
    "    \n",
    "    #clean_keyword = cleaning(row.keywords)\n",
    "    \n",
    "    for keyword in row.keywords:\n",
    "        print(f\"keyword = {keyword}\", end = \", \")\n",
    "        PARAMS[\"srsearch\"] = keyword\n",
    "        R = S.get(url = URL, params = PARAMS)\n",
    "        DATA = R.json()\n",
    "        \n",
    "        try:\n",
    "            target = DATA['query']['search'][0]['title']\n",
    "    \n",
    "        except:\n",
    "            print(\"There is no target, coninue\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        score = get_STS(keyword, target)\n",
    "        \n",
    "        if score >= 0.5:\n",
    "            print(f\"target = {target}, score= {score}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #if DATA['query']['search'][0]['title'] == keyword\n",
    "        \n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea0c19",
   "metadata": {},
   "source": [
    "# Cosine Simiarity vs Semantic Textual Similarity(Pearson Correlation)\n",
    "\n",
    "https://huggingface.co/tasks/sentence-similarity\n",
    "\n",
    "That's why I decided to use STS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07817e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_cosine_similarity(sen1, sen2):\n",
    "    \n",
    "    sent = (sen1, sen2)\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sent)\n",
    "    \n",
    "    return cosine_similarity(tfidf_matrix[0:1],tfidf_matrix[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1719477",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_token = \"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\"\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def get_STS(sen1, sen2):\n",
    "    data = query(\n",
    "        {\n",
    "            \"inputs\": {\n",
    "                \"source_sentence\": sen1,\n",
    "                \"sentences\":[sen2]\n",
    "            }\n",
    "        })\n",
    "    return data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1 = 'Language Models'\n",
    "sen2 = 'Language model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f17a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cosine_similarity(sen1, sen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be948a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_STS(sen1, sen2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
